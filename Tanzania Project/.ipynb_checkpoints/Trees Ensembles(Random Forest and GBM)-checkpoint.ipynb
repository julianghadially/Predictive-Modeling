{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['region', 'amount_tsh', 'funder', 'gps_height', 'installer',\n",
       "       'longitude', 'latitude', 'wpt_name', 'num_private', 'basin', 'lga',\n",
       "       'population', 'public_meeting', 'scheme_management', 'permit',\n",
       "       'extraction_type_class', 'management', 'payment', 'water_quality',\n",
       "       'quantity', 'source', 'source_class', 'waterpoint_type',\n",
       "       'waterpoint_type_group', 'age', 'loc_type', 'employment_rate',\n",
       "       'popdeath_rate', 'status_group', 'crime_rating'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Data preprocessing\n",
    "train = pd.read_csv(\"Data/data_train_pre.csv\", )\n",
    "test = pd.read_csv(\"Data/data_test_pre.csv\", )\n",
    "\n",
    "train[train['scheme_management']==\"None\"]['scheme_management'] = \"unknown\"\n",
    "test[test['scheme_management']==\"None\"]['scheme_management'] = \"unknown\"\n",
    "train=train.drop('extraction_type',axis=1,inplace=False)\n",
    "\n",
    "test=test.drop('extraction_type',axis=1,inplace=False)\n",
    "train=train.drop(['Unnamed: 0','id','date_recorded','quality_group','quantity_group', 'management_group','source_type','management_group','extraction_type_group',\n",
    "                  'district_code','region_code','payment_type','scheme_name'],axis=1,inplace=False)\n",
    "test=test.drop(['Unnamed: 0','date_recorded','quality_group','quantity_group', 'management_group','source_type','management_group','extraction_type_group',\n",
    "                  'district_code','region_code','payment_type','scheme_name'],axis=1,inplace=False)\n",
    "count=train['status_group'].value_counts()\n",
    "\n",
    "count\n",
    "\n",
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14850"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating train and hold out data set\n",
    "import random\n",
    "length=len(train)\n",
    "rows = random.sample(train.index, length/2)\n",
    "train_s1 = train.ix[rows]\n",
    "count=train_s1['status_group'].value_counts()\n",
    "train_s3 = train.drop(rows)\n",
    "length=len(train_s3)\n",
    "rows = random.sample(train_s3.index, length/2)\n",
    "train_s2 = train_s3.ix[rows]\n",
    "holdout=train_s3.drop(rows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count\n",
    "len(train_s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functional                 5246\n",
       "non functional             3626\n",
       "functional needs repair    2218\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing the downsampling fucntion\n",
    "def dnsmpl(xtrain,count,p):\n",
    "    train=xtrain\n",
    "    positive=count['functional needs repair']\n",
    "    negative=int(positive*((1-p)/p))\n",
    "    train_0=train[train['status_group']<>'functional needs repair']\n",
    "    train_1=train[train['status_group']=='functional needs repair']\n",
    "    train_0_down_sampled = train_0.sample( negative)\n",
    "    train_final=train_1.append(train_0_down_sampled)\n",
    "    return(train_final)\n",
    "\n",
    "y=train_s2['status_group']\n",
    "y_binary=pd.get_dummies(y)\n",
    "y_bin=y_binary['functional needs repair']\n",
    "train1=train_s1.drop('status_group',axis=1,inplace=False)\n",
    "train_s2_dum=pd.get_dummies(train_s2)\n",
    "\n",
    "\n",
    "x=dnsmpl(train_s1,count,0.20)\n",
    "x['status_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-916752cc4d72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcolumns1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxtrain_dum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcolumns2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_dum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumns2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mprediction_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_s2_dum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprediction_train_dummy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anchit\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1795\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1797\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anchit\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1802\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1804\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionaility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anchit\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1080\u001b[0m         \u001b[1;34m\"\"\" return the cached item, item represents a label indexer \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m         \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'set'"
     ]
    }
   ],
   "source": [
    "# finding the best downsampling value\n",
    "f1_score=[]  \n",
    "for p in xrange(10,60,10):\n",
    "    p1=float(p)/float(100)\n",
    "    xtrain=dnsmpl(train_s1,count,p1)\n",
    "    y1=xtrain['status_group']\n",
    "    xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "    model = RandomForestClassifier(n_estimators=200)\n",
    "    xtrain_dum=pd.get_dummies(xtrain)\n",
    "    columns=train_s2_dum.columns.values\n",
    "    columns1=xtrain_dum.columns.values\n",
    "    columns2=set(columns1).intersection(columns)\n",
    "    result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "    prediction_train = model.predict(train_s2_dum[list(columns2)])\n",
    "    prediction_train_dummy=pd.get_dummies(prediction_train)\n",
    "    prediction_train_dum=1-prediction_train_dummy[['functional','non functional']].max(axis=1)\n",
    "    f1=metrics.f1_score(y_bin, prediction_train_dum)\n",
    "    print 'f1',metrics.f1_score(y_bin, prediction_train_dum),'accuracy',metrics.accuracy_score(y, prediction_train)\n",
    "    f1_score.append(f1)\n",
    "lr_results1=pd.Series(f1_score) \n",
    "lr_results_best=lr_results1.order(ascending=False).index[0]\n",
    "print 'Best parameter for under sampling', lr_results_best \n",
    "lr_results1.order(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.385697538101 accuracy 0.793602693603\n",
      "f1 0.395061728395 accuracy 0.794545454545\n",
      "f1 0.388463802237 accuracy 0.793468013468\n",
      "f1 0.39858490566 accuracy 0.794276094276\n",
      "f1 0.394844756883 accuracy 0.793535353535\n",
      "Best parameter for under sampling 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    0.398585\n",
       "1    0.395062\n",
       "4    0.394845\n",
       "2    0.388464\n",
       "0    0.385698\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the optimal trees\n",
    "f1_score=[]  \n",
    "for p in xrange(100,600,100):\n",
    "    #p1=float(p)/float(100)\n",
    "    xtrain=train_s1\n",
    "    y1=xtrain['status_group']\n",
    "    xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "    model = RandomForestClassifier(n_estimators=p)\n",
    "    xtrain_dum=pd.get_dummies(xtrain)\n",
    "    columns=train_s2_dum.columns.values\n",
    "    columns1=xtrain_dum.columns.values\n",
    "    columns2=set(columns).intersection(columns1)\n",
    "    result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "    prediction_train = model.predict(train_s2_dum[list(columns2)])\n",
    "    prediction_train_dummy=pd.get_dummies(prediction_train)\n",
    "    prediction_train_dum=1-prediction_train_dummy[['functional','non functional']].max(axis=1)\n",
    "    f1=metrics.f1_score(y_bin, prediction_train_dum)\n",
    "    print 'f1',metrics.f1_score(y_bin, prediction_train_dum),'accuracy',metrics.accuracy_score(y, prediction_train)\n",
    "    f1_score.append(f1)\n",
    "lr_results1=pd.Series(f1_score) \n",
    "lr_results_best=lr_results1.order(ascending=False).index[0]\n",
    "print 'Best parameter for under sampling', lr_results_best \n",
    "lr_results1.order(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.387936132466 accuracy 0.793872053872\n",
      "f1 0.389959136019 accuracy 0.791851851852\n",
      "f1 0.397196261682 accuracy 0.794747474747\n",
      "Best parameter for under sampling 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    0.398585\n",
       "7    0.397196\n",
       "1    0.395062\n",
       "4    0.394845\n",
       "6    0.389959\n",
       "2    0.388464\n",
       "5    0.387936\n",
       "0    0.385698\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the optimal parameters\n",
    "c=['auto','log2',None]\n",
    "for p in c:\n",
    "    #p1=float(p)/float(100)\n",
    "    xtrain=train_s1\n",
    "    y1=xtrain['status_group']\n",
    "    xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "    model = RandomForestClassifier(max_features=p,n_estimators=300 )\n",
    "    xtrain_dum=pd.get_dummies(xtrain)\n",
    "    columns=train_s2_dum.columns.values\n",
    "    columns1=xtrain_dum.columns.values\n",
    "    columns2=set(columns).intersection(columns1)\n",
    "    result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "    prediction_train = model.predict(train_s2_dum[list(columns2)])\n",
    "    prediction_train_dummy=pd.get_dummies(prediction_train)\n",
    "    prediction_train_dum=1-prediction_train_dummy[['functional','non functional']].max(axis=1)\n",
    "    f1=metrics.f1_score(y_bin, prediction_train_dum)\n",
    "    print 'f1',metrics.f1_score(y_bin, prediction_train_dum),'accuracy',metrics.accuracy_score(y, prediction_train)\n",
    "    f1_score.append(f1)\n",
    "lr_results1=pd.Series(f1_score) \n",
    "lr_results_best=lr_results1.order(ascending=False).index[0]\n",
    "print 'Best parameter for under sampling', lr_results_best \n",
    "lr_results1.order(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###random forest with PCA\n",
    "train_dum=pd.get_dummies(train1)\n",
    "test_dum=pd.get_dummies(test)\n",
    "columns=test_dum.columns.values\n",
    "columns1=train_dum.columns.values\n",
    "columns2=set(columns).intersection(columns1)\n",
    "train_dum=train_dum[list(columns2)]\n",
    "test_dum=test_dum[list(columns2)]\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "print len(columns2)\n",
    "train_dum_scale=scale(train_dum)\n",
    "test_dum_scale=scale(test_dum)\n",
    "pca = PCA(n_components=100)\n",
    "pca_fit=pca.fit(train_dum_scale)\n",
    "train_pca=pca_fit.transform(train_dum_scale)\n",
    "test_pca=pca_fit.transform(test_dum_scale)\n",
    "pca.explained_variance_ratio_ \n",
    "train_pca=np.matrix(train_pca)\n",
    "train_pca=pd.DataFrame(train_pca)\n",
    "test_pca=np.matrix(test_pca)\n",
    "test_pca=pd.DataFrame(test_pca)\n",
    "model = RandomForestClassifier(n_estimators=1000,oob_score=True)\n",
    "result = model.fit(train_dum, y)\n",
    "out_of_bag_prediction_for_x = model.oob_score_\n",
    "print(out_of_bag_prediction_for_x)\n",
    "\n",
    "prediction_train = model.predict(test_dum[list(columns2)])\n",
    "print test.columns.values\n",
    "test['status_group']=prediction_train\n",
    "results=test[['id','status_group']]\n",
    "results.to_csv('E:/results_rf_1000.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.35928877989 accuracy 0.774074074074\n",
      "f1 0.364787111623 accuracy 0.77797979798\n",
      "f1 0.361047835991 accuracy 0.775892255892\n",
      "f1 0.360859728507 accuracy 0.774949494949\n",
      "f1 0.360859728507 accuracy 0.774949494949\n",
      "Best parameter for under sampling 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.364787\n",
       "2    0.361048\n",
       "4    0.360860\n",
       "3    0.360860\n",
       "0    0.359289\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "f1_score=[]  \n",
    "for p in xrange(100,600,100):\n",
    "    #p1=float(p)/float(100)\n",
    "    xtrain=train_s1\n",
    "    y1=xtrain['status_group']\n",
    "    xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "    model = GradientBoostingClassifier(n_estimators=p, learning_rate=1.0, random_state=0)\n",
    "    xtrain_dum=pd.get_dummies(xtrain)\n",
    "    columns=train_s2_dum.columns.values\n",
    "    columns1=xtrain_dum.columns.values\n",
    "    columns2=set(columns).intersection(columns1)\n",
    "    result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "    prediction_train = model.predict(train_s2_dum[list(columns2)])\n",
    "    prediction_train_dummy=pd.get_dummies(prediction_train)\n",
    "    prediction_train_dum=1-prediction_train_dummy[['functional','non functional']].max(axis=1)\n",
    "    f1=metrics.f1_score(y_bin, prediction_train_dum)\n",
    "    print 'f1',metrics.f1_score(y_bin, prediction_train_dum),'accuracy',metrics.accuracy_score(y, prediction_train)\n",
    "    f1_score.append(f1)\n",
    "lr_results1=pd.Series(f1_score) \n",
    "lr_results_best=lr_results1.order(ascending=False).index[0]\n",
    "print 'Best parameter for under sampling', lr_results_best \n",
    "lr_results1.order(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.361081081081 accuracy 0.769831649832\n",
      "f1 0.37911227154 accuracy 0.772929292929\n",
      "f1 0.367502726281 accuracy 0.779865319865\n",
      "f1 0.383167220377 accuracy 0.785185185185\n",
      "f1 0.400671516508 accuracy 0.788080808081\n",
      "f1 0.387451843698 accuracy 0.783973063973\n",
      "Best parameter for under sampling 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4    0.400672\n",
       "5    0.387452\n",
       "3    0.383167\n",
       "1    0.379112\n",
       "2    0.367503\n",
       "0    0.361081\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the optimal depth of trees\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "f1_score=[]  \n",
    "for p in xrange(5,35,5):\n",
    "    #p1=float(p)/float(100)\n",
    "    xtrain=train_s1\n",
    "    y1=xtrain['status_group']\n",
    "    xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "    model = GradientBoostingClassifier(n_estimators=100,max_depth=p, learning_rate=1.0, random_state=0)\n",
    "    xtrain_dum=pd.get_dummies(xtrain)\n",
    "    columns=train_s2_dum.columns.values\n",
    "    columns1=xtrain_dum.columns.values\n",
    "    columns2=set(columns).intersection(columns1)\n",
    "    result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "    prediction_train = model.predict(train_s2_dum[list(columns2)])\n",
    "    prediction_train_dummy=pd.get_dummies(prediction_train)\n",
    "    prediction_train_dum=1-prediction_train_dummy[['functional','non functional']].max(axis=1)\n",
    "    f1=metrics.f1_score(y_bin, prediction_train_dum)\n",
    "    print 'f1',metrics.f1_score(y_bin, prediction_train_dum),'accuracy',metrics.accuracy_score(y, prediction_train)\n",
    "    f1_score.append(f1)\n",
    "lr_results1=pd.Series(f1_score) \n",
    "lr_results_best=lr_results1.order(ascending=False).index[0]\n",
    "print 'Best parameter for under sampling', lr_results_best \n",
    "lr_results1.order(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##checking on the training set in driven data org\n",
    "test.dum=pd.get_dummies(test)\n",
    "\n",
    "xtrain=train_s1\n",
    "y1=xtrain['status_group']\n",
    "xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "model = GradientBoostingClassifier(n_estimators=100,max_depth=15, learning_rate=1.0, random_state=0)\n",
    "xtrain_dum=pd.get_dummies(xtrain)\n",
    "columns=test.dum.columns.values\n",
    "columns1=xtrain_dum.columns.values\n",
    "columns2=set(columns).intersection(columns1)\n",
    "result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "prediction_train = model.predict(test.dum[list(columns2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_train\n",
    "test['status_group']=prediction_train\n",
    "results=test[['status_group','id']]\n",
    "\n",
    "results.to_csv('E:/results_gbm_100.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
