{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['region', 'amount_tsh', 'funder', 'gps_height', 'installer',\n",
       "       'longitude', 'latitude', 'wpt_name', 'num_private', 'basin', 'lga',\n",
       "       'population', 'public_meeting', 'scheme_management', 'permit',\n",
       "       'extraction_type_class', 'management', 'payment', 'water_quality',\n",
       "       'quantity', 'source', 'source_class', 'waterpoint_type',\n",
       "       'waterpoint_type_group', 'age', 'loc_type', 'employment_rate',\n",
       "       'popdeath_rate', 'status_group', 'crime_rating'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Data preprocessing\n",
    "train = pd.read_csv(\"D:/Fall/APM/project/data_train_pre.csv\", )\n",
    "test = pd.read_csv(\"D:/Fall/APM/project/data_test_pre.csv\", )\n",
    "\n",
    "train[train['scheme_management']==\"None\"]['scheme_management'] = \"unknown\"\n",
    "test[test['scheme_management']==\"None\"]['scheme_management'] = \"unknown\"\n",
    "train=train.drop('extraction_type',axis=1,inplace=False)\n",
    "\n",
    "test=test.drop('extraction_type',axis=1,inplace=False)\n",
    "train=train.drop(['Unnamed: 0','id','date_recorded','quality_group','quantity_group', 'management_group','source_type','management_group','extraction_type_group',\n",
    "                  'district_code','region_code','payment_type','scheme_name'],axis=1,inplace=False)\n",
    "test=test.drop(['Unnamed: 0','date_recorded','quality_group','quantity_group', 'management_group','source_type','management_group','extraction_type_group',\n",
    "                  'district_code','region_code','payment_type','scheme_name'],axis=1,inplace=False)\n",
    "count=train['status_group'].value_counts()\n",
    "\n",
    "count\n",
    "\n",
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functional                 16068\n",
       "non functional             11485\n",
       "functional needs repair     2147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating train and hold out data set\n",
    "import random\n",
    "length=len(train)\n",
    "rows = random.sample(train.index, length/2)\n",
    "train_s1 = train.ix[rows]\n",
    "count=train_s1['status_group'].value_counts()\n",
    "train_s2 = train.drop(rows)\n",
    "\n",
    "count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writing the downsampling fucntion\n",
    "def dnsmpl(xtrain,count,p):\n",
    "    train=xtrain\n",
    "    positive=count['functional needs repair']\n",
    "    negative=int(positive*((1-p)/p))\n",
    "    train_0=train[train['status_group']<>'functional needs repair']\n",
    "    train_1=train[train['status_group']=='functional needs repair']\n",
    "    train_0_down_sampled = train_0.sample( negative)\n",
    "    train_final=train_1.append(train_0_down_sampled)\n",
    "    return(train_final)\n",
    "\n",
    "y=train_s2['status_group']\n",
    "train1=train_s1.drop('status_group',axis=1,inplace=False)\n",
    "train_s2_dum=pd.get_dummies(train_s2)\n",
    "y_binary=pd.get_dummies(y)\n",
    "y_bin=y_binary['functional needs repair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 (array([ 0.92808081,  0.        ]), array([ 1.,  0.]), array([ 0.96269908,  0.        ]), array([27564,  2136], dtype=int64)) accuracy 0.391582491582\n",
      "f1 (array([ 0.9280687,  0.       ]), array([ 0.9998186,  0.       ]), array([ 0.9626085,  0.       ]), array([27564,  2136], dtype=int64)) accuracy 0.471515151515\n",
      "f1 (array([ 0.92770307,  0.02777778]), array([ 0.99111159,  0.00327715]), array([ 0.95835964,  0.00586265]), array([27564,  2136], dtype=int64)) accuracy 0.547306397306\n",
      "f1 (array([ 0.92189349,  0.06871166]), array([ 0.33913801,  0.62921348]), array([ 0.49586251,  0.12389381]), array([27564,  2136], dtype=int64)) accuracy 0.22404040404\n",
      "f1 (array([ 0.90228359,  0.07017292]), array([ 0.06163837,  0.91385768]), array([ 0.11539376,  0.13033753]), array([27564,  2136], dtype=int64)) accuracy 0.0963636363636\n",
      "Best parameter for under sampling 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4    0.130338\n",
       "3    0.123894\n",
       "2    0.005863\n",
       "1    0.000000\n",
       "0    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the best downsampling value\n",
    "f1_score=[]  \n",
    "for p in xrange(10,60,10):\n",
    "    p1=float(p)/float(100)\n",
    "    xtrain=dnsmpl(train_s1,count,p1)\n",
    "    y1=xtrain['status_group']\n",
    "    xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "    model = RandomForestClassifier(n_estimators=200)\n",
    "    xtrain_dum=pd.get_dummies(xtrain)\n",
    "    result = model.fit(xtrain_dum, y1)\n",
    "    columns=train_s2_dum.columns.values\n",
    "    columns1=xtrain_dum.columns.values\n",
    "    columns2=set(columns1).intersection(columns)\n",
    "    prediction_train = model.predict(train_s2_dum[list(columns2)])\n",
    "    prediction_train_dummy=pd.get_dummies(prediction_train)\n",
    "    prediction_train_dum=1-prediction_train_dummy[['functional','non functional']].max(axis=1)\n",
    "    f1=metrics.f1_score(y_bin, prediction_train_dum)\n",
    "    print 'f1',metrics.precision_recall_fscore_support(y_bin, prediction_train_dum),'accuracy',metrics.accuracy_score(y, prediction_train)\n",
    "    f1_score.append(f1)\n",
    "lr_results1=pd.Series(f1_score) \n",
    "lr_results_best=lr_results1.order(ascending=False).index[0]\n",
    "print 'Best parameter for under sampling', lr_results_best \n",
    "lr_results1.order(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.78826518541 accuracy 0.794175084175\n",
      "f1 0.787674782482 accuracy 0.793535353535\n",
      "f1 0.790208408984 accuracy 0.795858585859\n",
      "f1 0.789447123152 accuracy 0.795218855219\n",
      "f1 0.78883953071 accuracy 0.794511784512\n",
      "Best parameter for under sampling 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    0.790208\n",
       "3    0.789447\n",
       "4    0.788840\n",
       "0    0.788265\n",
       "1    0.787675\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the optimal trees\n",
    "f1_score=[]  \n",
    "for p in xrange(100,600,100):\n",
    "    #p1=float(p)/float(100)\n",
    "    xtrain=train_s1\n",
    "    y1=xtrain['status_group']\n",
    "    xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "    model = RandomForestClassifier(n_estimators=p)\n",
    "    xtrain_dum=pd.get_dummies(xtrain)\n",
    "    columns=train_s2_dum.columns.values\n",
    "    columns1=xtrain_dum.columns.values\n",
    "    columns2=set(columns).intersection(columns1)\n",
    "    result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "    prediction_train = model.predict(train_s2_dum[list(columns2)])\n",
    "    #prediction_train_dummy=pd.get_dummies(prediction_train)\n",
    "    #prediction_train_dum=1-prediction_train_dummy[['functional','non functional']].max(axis=1)\n",
    "    f1=metrics.f1_score(y, prediction_train)\n",
    "    print 'f1',metrics.f1_score(y, prediction_train),'accuracy',metrics.accuracy_score(y, prediction_train)\n",
    "    f1_score.append(f1)\n",
    "lr_results1=pd.Series(f1_score) \n",
    "lr_results_best=lr_results1.order(ascending=False).index[0]\n",
    "print 'Best parameter for under sampling', lr_results_best \n",
    "lr_results1.order(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.788908301282 accuracy 0.794781144781\n",
      "f1 0.787675493799 accuracy 0.793367003367\n",
      "f1 0.787631524432 accuracy 0.793400673401\n",
      "Best parameter for under sampling 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    0.790208\n",
       "3    0.789447\n",
       "5    0.788908\n",
       "4    0.788840\n",
       "0    0.788265\n",
       "6    0.787675\n",
       "1    0.787675\n",
       "7    0.787632\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the optimal parameters\n",
    "c=['auto','log2',None]\n",
    "for p in c:\n",
    "    #p1=float(p)/float(100)\n",
    "    xtrain=train_s1\n",
    "    y1=xtrain['status_group']\n",
    "    xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "    model = RandomForestClassifier(max_features=p,n_estimators=300 )\n",
    "    xtrain_dum=pd.get_dummies(xtrain)\n",
    "    columns=train_s2_dum.columns.values\n",
    "    columns1=xtrain_dum.columns.values\n",
    "    columns2=set(columns).intersection(columns1)\n",
    "    result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "    prediction_train = model.predict(train_s2_dum[list(columns2)])\n",
    "    #prediction_train_dummy=pd.get_dummies(prediction_train)\n",
    "    #prediction_train_dum=1-prediction_train_dummy[['functional','non functional']].max(axis=1)\n",
    "    f1=metrics.f1_score(y, prediction_train)\n",
    "    print 'f1',metrics.f1_score(y, prediction_train),'accuracy',metrics.accuracy_score(y, prediction_train)\n",
    "    f1_score.append(f1)\n",
    "lr_results1=pd.Series(f1_score) \n",
    "lr_results_best=lr_results1.order(ascending=False).index[0]\n",
    "print 'Best parameter for under sampling', lr_results_best \n",
    "lr_results1.order(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###random forest with PCA\n",
    "train_dum=pd.get_dummies(train1)\n",
    "test_dum=pd.get_dummies(test)\n",
    "columns=test_dum.columns.values\n",
    "columns1=train_dum.columns.values\n",
    "columns2=set(columns).intersection(columns1)\n",
    "train_dum=train_dum[list(columns2)]\n",
    "test_dum=test_dum[list(columns2)]\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "print len(columns2)\n",
    "train_dum_scale=scale(train_dum)\n",
    "test_dum_scale=scale(test_dum)\n",
    "pca = PCA(n_components=100)\n",
    "pca_fit=pca.fit(train_dum_scale)\n",
    "train_pca=pca_fit.transform(train_dum_scale)\n",
    "test_pca=pca_fit.transform(test_dum_scale)\n",
    "pca.explained_variance_ratio_ \n",
    "train_pca=np.matrix(train_pca)\n",
    "train_pca=pd.DataFrame(train_pca)\n",
    "test_pca=np.matrix(test_pca)\n",
    "test_pca=pd.DataFrame(test_pca)\n",
    "model = RandomForestClassifier(n_estimators=1000,oob_score=True)\n",
    "result = model.fit(train_dum, y)\n",
    "out_of_bag_prediction_for_x = model.oob_score_\n",
    "print(out_of_bag_prediction_for_x)\n",
    "\n",
    "prediction_train = model.predict(test_dum[list(columns2)])\n",
    "print test.columns.values\n",
    "test['status_group']=prediction_train\n",
    "results=test[['id','status_group']]\n",
    "results.to_csv('E:/results_rf_1000.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.76361807938 accuracy 0.773333333333\n",
      "f1 0.767487223639 accuracy 0.775151515152\n",
      "f1 0.767505410009 accuracy 0.77468013468\n",
      "f1 0.769702412826 accuracy 0.776464646465\n",
      "f1 0.767372272309 accuracy 0.773939393939\n",
      "Best parameter for under sampling 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    0.769702\n",
       "2    0.767505\n",
       "1    0.767487\n",
       "4    0.767372\n",
       "0    0.763618\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "f1_score=[]  \n",
    "for p in xrange(100,600,100):\n",
    "    #p1=float(p)/float(100)\n",
    "    xtrain=train_s1\n",
    "    y1=xtrain['status_group']\n",
    "    xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "    model = GradientBoostingClassifier(n_estimators=p, learning_rate=1.0, random_state=0)\n",
    "    xtrain_dum=pd.get_dummies(xtrain)\n",
    "    columns=train_s2_dum.columns.values\n",
    "    columns1=xtrain_dum.columns.values\n",
    "    columns2=set(columns).intersection(columns1)\n",
    "    result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "    prediction_train = model.predict(train_s2_dum[list(columns2)])\n",
    "    #prediction_train_dummy=pd.get_dummies(prediction_train)\n",
    "    #prediction_train_dum=1-prediction_train_dummy[['functional','non functional']].max(axis=1)\n",
    "    f1=metrics.f1_score(y, prediction_train)\n",
    "    print 'f1',metrics.f1_score(y, prediction_train),'accuracy',metrics.accuracy_score(y, prediction_train)\n",
    "    f1_score.append(f1)\n",
    "lr_results1=pd.Series(f1_score) \n",
    "lr_results_best=lr_results1.order(ascending=False).index[0]\n",
    "print 'Best parameter for under sampling', lr_results_best \n",
    "lr_results1.order(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.848596383147 accuracy 0.851616161616\n",
      "f1 0.885869044629 accuracy 0.886936026936\n",
      "f1 0.887151324178 accuracy 0.88835016835\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-41d378ef10be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcolumns1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxtrain_dum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcolumns2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_dum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mprediction_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_s2_dum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#prediction_train_dummy=pd.get_dummies(prediction_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anchit\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m--> 980\u001b[1;33m                                     begin_at_stage, monitor)\n\u001b[0m\u001b[0;32m    981\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anchit\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1039\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m                                      random_state)\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anchit\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, criterion, splitter, random_state)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 766\u001b[1;33m                      check_input=False)\n\u001b[0m\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Anchit\\Anaconda\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    302\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# finding the optimal depth of trees\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "f1_score=[]  \n",
    "for p in xrange(5,35,5):\n",
    "    #p1=float(p)/float(100)\n",
    "    xtrain=train_s1\n",
    "    y1=xtrain['status_group']\n",
    "    xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "    model = GradientBoostingClassifier(n_estimators=100,max_depth=p, learning_rate=1.0, random_state=0)\n",
    "    xtrain_dum=pd.get_dummies(xtrain)\n",
    "    columns=train_s2_dum.columns.values\n",
    "    columns1=xtrain_dum.columns.values\n",
    "    columns2=set(columns).intersection(columns1)\n",
    "    result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "    prediction_train = model.predict(train_s2_dum[list(columns2)])\n",
    "    #prediction_train_dummy=pd.get_dummies(prediction_train)\n",
    "    #prediction_train_dum=1-prediction_train_dummy[['functional','non functional']].max(axis=1)\n",
    "    f1=metrics.f1_score(y, prediction_train)\n",
    "    print 'f1',metrics.f1_score(y, prediction_train),'accuracy',metrics.accuracy_score(y, prediction_train)\n",
    "    f1_score.append(f1)\n",
    "lr_results1=pd.Series(f1_score) \n",
    "lr_results_best=lr_results1.order(ascending=False).index[0]\n",
    "print 'Best parameter for under sampling', lr_results_best \n",
    "lr_results1.order(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##checking on the training set in driven data org\n",
    "test.dum=pd.get_dummies(test)\n",
    "\n",
    "xtrain=train_s1\n",
    "y1=xtrain['status_group']\n",
    "xtrain=xtrain.drop('status_group',axis=1,inplace=False)\n",
    "model = GradientBoostingClassifier(n_estimators=100,max_depth=15, learning_rate=1.0, random_state=0)\n",
    "xtrain_dum=pd.get_dummies(xtrain)\n",
    "columns=test.dum.columns.values\n",
    "columns1=xtrain_dum.columns.values\n",
    "columns2=set(columns).intersection(columns1)\n",
    "result = model.fit(xtrain_dum[list(columns2)], y1)\n",
    "prediction_train = model.predict(test.dum[list(columns2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_train\n",
    "test['status_group']=prediction_train\n",
    "results=test[['status_group','id']]\n",
    "\n",
    "results.to_csv('E:/results_gbm_100.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
