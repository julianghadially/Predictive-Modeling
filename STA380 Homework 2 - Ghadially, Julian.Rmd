---
title: "Homework 2"
author: "Julian Ghadially"
date: "August 17, 2015"
output: html_document
---

##Problem 1: Flights at Austin Bergstrom International Airport

Analysis shows that departure delay time at Austin Bergstrom Airport increases as it gets later in the day, see figure one. 
Furthermore, United Airlines is drastically less punctual than the other major airlines. This is not due to a smaller number of flights since our data contains 1866 United Airlines flights.

```{r, echo = FALSE, results = 'hide', include = FALSE, warning=FALSE}
library(ggplot2)
library(timeDate)
library(mosaic)
AUS = read.csv('Data/Abia.csv')
AUS$CRSDepTime = as.character(AUS$CRSDepTime%/%100) # round down the Hour

#hist(AUS$DepDelay)
#head(AUS)
#head(AUS$DepDelay%%60)
#bwplot(DepDelay~cut(CRSDepTime, 5), data = AUS)

#strptime(as.character(AUS$CRSDepTime), format = "%H%M")[1:10]

#bwplot(DepDelay~factor(CRSDepTime), data = AUS)

df = aggregate(DepDelay~CRSDepTime, data = AUS, FUN = mean)
df = data.frame('DepHour' = df[[1]], 'Delay' = df[[2]], 'Airline' = rep('All', length(df[[2]])))

df = rbind(df[16:20,], df[2:15,])
l1 = df[[1]]
l2 = df[[2]]
df2 = data.frame('DepHour' = l1, 'Delay' = l2)
qplot(x = df2$DepHour, y = df2$Delay, xlab = 'Departure Hour', ylab = 'Average Delay (min)', main = 'Delays by hour') + geom_point() + geom_bar(stat = 'identity')


df = aggregate(DepDelay~CRSDepTime + UniqueCarrier, data = AUS, FUN = mean)
df = data.frame('DepHour' = df[[1]], 'Delay' = df[[3]], 'Airline' = df$UniqueCarrier)

mask = df$Airline %in% c('AA','CO','DL','UA')
df = df[mask,]



```

```{r, echo = FALSE, warning = FALSE, message = FALSE}
qplot(x = df2$DepHour, y = df2$Delay, xlab = 'Departure Hour', ylab = 'Average Delay) (min)', main = 'Total Delays by hour') + geom_point() + geom_bar(stat = 'identity')
ggplot(data = df, aes(DepHour, Delay)) + geom_line(aes(color = Airline, group = Airline)) + xlab('Departure Hour') + ylab('Average Delay (min)') + ggtitle('Delays by Airline') + scale_color_discrete(name="Airlines", breaks=c('AA','CO','DL','UA'), labels=c("American", "Continental", "Delta", "United")) + scale_colour_manual(values = c('red','blue','orange','black'))
```


##Problem 2: Reuters Author Attribution



**Bag of Words Model**

In this exercise, works from 50 authors are analysed in order to attribute authorship to unknown text files from the same set of authors. Text is represented using a bag of words model. In the bag of words model, each document is a bag with counts associated with each word from the corpora of all authors. In order to best represent the text using bag of words, common words are ommitted, since they don't add as much value as the uncommon words. Sparse words are also ommitted. 

**Calculating Multinomial Probability Vector**

The multinomial probability vector is made up of the likelihood of each word for each author involved. In order to do this, the document term matrix is condensed into counts of words per author. These counts are converted into likelihoods by dividing by the total corpora word count. A laplacian smoothing term of 1/2500 is added to each count in order to estimate the likelihood of words unseen by the training corpora.

**Handling words in the test set that are not present in the training data**

The naive-bayes model is trained solely on documents from the training text, however, words from the test set are included in the document term matrices for the training set. the likelihoods of the unseen test words in the training set are equal to the value dictated by Laplacian smoothing. Because this is likelihood we would assign them if they were in seperate document term matrices, it is acceptable to include them in the set of training words.
Another way to handle the unseen test words would be to use only the intersection of words between the two corpora. However, this biases the training set to include information from the test set through omission of words unique to the training set. 

**Naive-Bayes Performance**

The Naive-Bayes model described above attributed the correct author **57%** of the time. This is very good compared to a 2% chance of getting the correct author with no model at all.
Also, author attribution varied for different authors. the table below displays accuracy for each author. Six authors were very troublesome to predict, with classification scores of 20% or less.



```{r, echo = FALSE, results = FALSE, include = FALSE}
#library(XML)
library(tm)
source('Data/textutils.R')
stories.and.authors.list = read.directory.of.directories('Data/ReutersC50/C50train')
all.stories = stories.and.authors.list[[1]]
authors = stories.and.authors.list[2]
authors_train = authors[[1]]
authors_unique = unique(authors_train)

test.stories.and.authors.list = read.directory.of.directories('Data/ReutersC50/C50test', first = 25)
test.all.stories = test.stories.and.authors.list[[1]]
test.authors = test.stories.and.authors.list[2]
test.authors = test.authors[[1]]

all.stories = append(all.stories, test.all.stories)



my_corpus = Corpus(VectorSource(all.stories))
my_corpus = tm_map(my_corpus, content_transformer(tolower))
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers))
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation))
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace))
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM = DocumentTermMatrix(my_corpus)
#class(DTM)
#inspect(DTM[1:10,1:20])
DTM = removeSparseTerms(DTM, 0.975)
X = as.matrix(DTM)

X_train = X[1:2500,]
X_test = X[2501:5000,]

smooth_count = 1/nrow(X_train)
w_authors_train = rowsum(X_train + smooth_count, authors_train)
#rowSums
#w_authors_train = w_all/rowSums(w_all)
w_authors_train = w_authors_train/sum(w_authors_train)
w_authors_train = log(w_authors_train)

#Naive-bayes
goodclassifications = 0
misclassifications = 0
for (j in 1:2500) {
  Bayes = NULL
  for (i in 1:50) {
    Bayes = append(Bayes, sum(X_test[j,]*w_authors_train[i,]))
  }
  attributed.author = which(Bayes == max(Bayes))
  attributed.author = authors_unique[attributed.author]
  #print(attributed.author)
  if (attributed.author == test.authors[j]) {
    goodclassifications = goodclassifications + 1
  }
  else{misclassifications = misclassifications + 1}
}
#goodclassifications/sum(goodclassifications, misclassifications)
```

```{r, echo = FALSE}
goodclassifications/sum(goodclassifications, misclassifications)
```


```{r, echo = FALSE}
goodclassifications = 0
misclassifications = 0
misclass_per_author = NULL
for (j in 1:2500) {
  Bayes = NULL
  for (i in 1:50) {
    Bayes = append(Bayes, sum(X_test[j,]*w_authors_train[i,]))
  }
  attributed.author = which(Bayes == max(Bayes))
  attributed.author = authors_unique[attributed.author]
  #print(attributed.author)
  if (attributed.author == test.authors[j]) {
    goodclassifications = goodclassifications + 1
  }
  else{misclassifications = misclassifications + 1}
  if (j%%50 == 0) {
    misclass_per_author = append(misclass_per_author, goodclassifications/sum(goodclassifications, misclassifications))
    misclassifications = 0
    goodclassifications = 0
  }
}
mpa.df = data.frame(x = authors_unique, y = misclass_per_author)
mpa.df
```


**Random Forest Model**

Naive Bayes did well predicting authors. Random Forests can also be applied to the same bag of words analysis. In order to do so, the bag of words model needs less variables. We can choose a subset of variables using principal component analysis (PCA). PCA generates 1422 principal components. However, 1422 variables for 2500 observations would produce too much variability error. Thus, we select the top principal components. The number of principal components should be selected using cross validation, for increased accuracy. 
Using 50 principal components, the random forest model is fit to the training data and used to predict on the test data. mtry is set to 4 and number of trees to 100. The percent of correct author attributions obtained is **66%.** This is higher than the Naive Bayes because Random Forest is using more important components dictated by PCA. As a result, the five authors that were difficult to predict are less difficult to predict. shown below is a table of prediction accuracy for each author by Random Forests.


```{r, echo = FALSE, result = FALSE, include = FALSE}
library(randomForest)
PC_X_train = prcomp(X_train, center = TRUE, scale = TRUE)
pr.var = PC_X_train$sdev^2
pr.ve = pr.var/sum(pr.var)



PC_X_test = prcomp(X_test, center = TRUE, scale = TRUE)


rf.tree = randomForest(x = PC_X_train$x[,1:50], y = factor(authors_train), ntree = 100, mtry = 4)
yhat = predict(rf.tree, newx = PC_X_test$x[,1:50])
sum(yhat == test.authors) / (sum(yhat!= test.authors) + sum(yhat == test.authors))

goodclassifications = 0
misclassifications = 0
misclass_per_author = NULL
for (i in 1:length(yhat)) {
  if (yhat[i] == test.authors[i]) {
    goodclassifications = goodclassifications + 1
  }
  else{misclassifications = misclassifications + 1}
  if (i%%50 == 0) {
    misclass_per_author = append(misclass_per_author, goodclassifications/sum(goodclassifications, misclassifications))
    misclassifications = 0
    goodclassifications = 0
  }
}
mpa.df = data.frame(x = authors_unique, y = misclass_per_author)

```
```{r, echo = FALSE}
mpa.df
```







##Problem 3: Association Rule Mining

Association between grocery items is important for grocery stores because it can help them understand what items are being bought together, which can in tern help them manage supply and appropriately place inventory and promotions. Associations for groceries from groceries.txt were gathered using a minimum support of .01 and confidence of .5. The  associations found are displayed below. 
This analysis points to two common items, milk and vegetables associated with particular types of products. Purchase of milk is associated with eggs and various dairy products, and purchase of "other vegetables"" is associated with purchase of various produce. These all have a support over .01, which means that 10% of all grocery carts contained these associations. They also all have a confidence greater than .5, which means that for grocery carts containing itemset A, over 50% contained the righthand side itemset. 

The drawback of the association rules found is that there might be a statistical dependence resulting from milk and vegetables being very common. Thus, we'd like to also find association rules for less common items. We can select for higher lifts, in order to consider items with less support. (Lift evaluates confidence divided by support).

When we select for higher lifts, we find many interesting association rules, which have high confidences. These are shown below under, "Using Lift." In particular we find that 90% of itemsets containing liquor and wine also contain bottled beer. Hamburger meat, whiped/sour cream, and yogurt is associated with butter. Also, popcorn and soda item sets are associated with salty snacks. 

These item sets make sense and are not as reliant on high statistical dependence found with popular items like milk and vegetables.






```{r, echo = FALSE, result = FALSE, include = FALSE}
library(arules)
groceries = readLines('Data/groceries.txt')
groceries = lapply(groceries, strsplit, split = ',', fixed = TRUE)
groceries = unlist(groceries, recursive = FALSE)
grocerytrans = as(groceries, 'transactions')
basketrules <- apriori(grocerytrans, 
	parameter=list(support=.01, confidence=.5, maxlen=4))
```

```{r, echo = FALSE}
inspect(basketrules)
```

###Using Lift

```{r, echo = FALSE, include = FALSE}
basketrules <- apriori(grocerytrans, 
	parameter=list(support=.001, confidence=.5, maxlen=4))
```
```{r, echo = FALSE}
inspect(subset(basketrules, subset = lift > 10))
```














