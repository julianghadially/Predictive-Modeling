---
title: "Homework 2"
author: "Julian Ghadially"
date: "August 17, 2015"
output: html_document
---
```{r, echo = FALSE, eval = FALSE}
#P2
#*you should use cv to test for number of PC's*
#P3
#**????????Should I say why I chose confidence ?????????????????? lift is confidence over the support of the right hand side i believe**
```




##Problem 1: Flights at Austin Bergstrom International Airport

Do a time delay plot by airline.

```{r, echo = FALSE}
library(ggplot2)
library(timeDate)
library(mosaic)
AUS = read.csv('Data/Abia.csv')
hist(AUS$DepDelay)
#head(AUS)
head(AUS$DepDelay%%60)
bwplot(DepDelay~cut(CRSDepTime, 5), data = AUS)

#strptime(as.character(AUS$CRSDepTime), format = "%H%M")[1:10]
AUS$CRSDepTime = as.character(AUS$CRSDepTime%/%100)
#bwplot(DepDelay~factor(CRSDepTime), data = AUS)
df = aggregate(DepDelay~CRSDepTime, data = AUS, FUN = mean)
df = data.frame('DepHour' = df[[1]], 'Delay' = df[[2]])

df = rbind(df[16:20,], df[2:15,])
l1 = df[[1]]
l2 = df[[2]]
df2 = data.frame('DepHour' = l1, 'Delay' = l2)
#qplot(x = df2$DepHour, y = df2$Delay, xlab = 'Departure Hour', ylab = 'Average Delay (min)', main = 'Delays by hour') + geom_point() + geom_line(stat = 'identity')

#ggplot2(data = df, aes(x = DepHour, y = Delay, group = ))
```



##Problem 2: Reuters Author Attribution



**Bag of Words Model**

In this exercise, works from 50 authors are analysed in order to attribute authorship to unknown text files from the same set of authors. Text is represented using a bag of words model. In the bag of words model, each document is a bag with counts associated with each word from the corpora of all authors. In order to best represent the text using bag of words, common words are ommitted, since they don't add as much value as the uncommon words. Sparse words are also ommitted. 

**Calculating Multinomial Probability Vector**

The multinomial probability vector is made up of the likelihood of each word for each author involved. In order to do this, the document term matrix is condensed into counts of words per author. These counts are converted into likelihoods by dividing by the total corpora word count. A laplacian smoothing term of 1/2500 is added to each count in order to estimate the likelihood of words unseen by the training corpora.

**Handling words in the test set that are not present in the training data**

The naive-bayes model is trained solely on documents from the training text, however, words from the test set are included in the document term matrices for the training set. the likelihoods of the unseen test words in the training set are equal to the value dictated by Laplacian smoothing. Because this is likelihood we would assign them if they were in seperate document term matrices, it is acceptable to include them in the set of training words.
Another way to handle the unseen test words would be to use only the intersection of words between the two corpora. However, this biases the training set to include information from the test set through omission of words unique to the training set. 

**Naive-Bayes Performance**

The Naive-Bayes model described above attributed the correct author **57%** of the time. This is very good compared to a 2% chance of getting the correct author with no model at all.
Also, author attribution varied for different authors. the table below displays accuracy for each author. Six authors were very troublesome to predict, with classification scores of 20% or less.



```{r, echo = FALSE, eval = FALSE}
'''library(tm)
source(\'Data/textutils.R\')
stories.and.authors.list = read.directory.of.directories(\'Data/ReutersC50/C50train\')
all.stories = stories.and.authors.list[[1]]
authors = stories.and.authors.list[2]
authors = authors[[1]]
authors_unique = unique(authors)

my_corpus = Corpus(VectorSource(all.stories))
my_corpus = tm_map(my_corpus, content_transformer(tolower))
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers))
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation))
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace))
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM = DocumentTermMatrix(my_corpus)
class(DTM)
inspect(DTM[1:10,1:20])
DTM = removeSparseTerms(DTM, 0.975)
X = as.matrix(DTM)

smooth_count = 1/nrow(X)
w_authors = rowsum(X + smooth_count, authors)
rowSums
#w_authors = w_all/rowSums(w_all)
w_authors = w_all/sum(w_all)
w_authors = log(w_authors)


###########################------------TEST---------------
test.stories.and.authors.list = read.directory.of.directories(\'Data/ReutersC50/C50test\')
test.all.stories = test.stories.and.authors.list[[1]]
test.authors = test.stories.and.authors.list[2]
test.authors = test.authors[[1]]

my_test_corpus = Corpus(VectorSource(test.all.stories))
my_test_corpus = tm_map(my_test_corpus, content_transformer(tolower))
my_test_corpus = tm_map(my_test_corpus, content_transformer(removeNumbers))
my_test_corpus = tm_map(my_test_corpus, content_transformer(removePunctuation))
my_test_corpus = tm_map(my_test_corpus, content_transformer(stripWhitespace))
my_test_corpus = tm_map(my_test_corpus, content_transformer(removeWords), stopwords("SMART"))

test.DTM = DocumentTermMatrix(my_test_corpus)
class(test.DTM)
inspect(test.DTM[1:10,1:20])
test.DTM = removeSparseTerms(test.DTM, 0.975)
X.test = as.matrix(test.DTM)


#Naive-bayes
for (j in 1:50) {
  Bayes = NULL
  for (i in 1:50) {
    Bayes = append(Bayes, sum(X.test[j,]*w_authors[i]))
  }
  attributed.author = which(Bayes == max(Bayes))
  attributed.author
  print(authors_unique[attributed.author])
}'''


#train = sample(1:len(X), len(X)*.7)
#X_train = x[train,]
#get likelihoods per author
#authors_unique = unique(authors)

#for (author in authors_unique) {
#  count_author = sum(authors == author)
#}
#prev_author = authors[1]
#author_idx_set = (authors == authors[1])
#num_author_docs = sum(authors == authors[1])
#author_word_count = sum(X[author_idx_set,])
#w_likelihoods_per_author = NULL
#w_likelihood_per_author = append(w_likelihood_per_author, w_all/author_word_count)
#j = 1


#for (i in 1:2500) {
#  if (prev_author != authors[i]) {
#    #w_likelihood_per_author[j] = colSums(X[i-num_author_docs:(i-1)] + smooth_count)
#    num_author_docs = sum(authors == authors[i])
#    author_idx_set = (authors == authors[i])
#    author_word_count = sum(X[author_idx_set,])
#  }
#  w_likelihood_per_author = append(w_likelihood_per_author, w_all/author_word_count)
#}
#all.stories.vec = LoW_to_countvector(all.stories)
#all.stories.std = standardize.ragged(all.stories.vec)
#all.stories.std = remove.singletons.ragged(all.stories.std)
#all.stories.DTM = make.BoW.frame(all.stories.std) #make document term matrix
```



```{r, echo = FALSE, result = FALSE}
#library(XML)
library(tm)
source('Data/textutils.R')
stories.and.authors.list = read.directory.of.directories('Data/ReutersC50/C50train')
all.stories = stories.and.authors.list[[1]]
authors = stories.and.authors.list[2]
authors_train = authors[[1]]
authors_unique = unique(authors_train)

test.stories.and.authors.list = read.directory.of.directories('Data/ReutersC50/C50test', first = 25)
test.all.stories = test.stories.and.authors.list[[1]]
test.authors = test.stories.and.authors.list[2]
test.authors = test.authors[[1]]

all.stories = append(all.stories, test.all.stories)



my_corpus = Corpus(VectorSource(all.stories))
my_corpus = tm_map(my_corpus, content_transformer(tolower))
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers))
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation))
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace))
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM = DocumentTermMatrix(my_corpus)
#class(DTM)
#inspect(DTM[1:10,1:20])
DTM = removeSparseTerms(DTM, 0.975)
X = as.matrix(DTM)

X_train = X[1:2500,]
X_test = X[2501:5000,]

smooth_count = 1/nrow(X_train)
w_authors_train = rowsum(X_train + smooth_count, authors_train)
#rowSums
#w_authors_train = w_all/rowSums(w_all)
w_authors_train = w_authors_train/sum(w_authors_train)
w_authors_train = log(w_authors_train)

#Naive-bayes
goodclassifications = 0
misclassifications = 0
for (j in 1:2500) {
  Bayes = NULL
  for (i in 1:50) {
    Bayes = append(Bayes, sum(X_test[j,]*w_authors_train[i,]))
  }
  attributed.author = which(Bayes == max(Bayes))
  attributed.author = authors_unique[attributed.author]
  #print(attributed.author)
  if (attributed.author == test.authors[j]) {
    goodclassifications = goodclassifications + 1
  }
  else{misclassifications = misclassifications + 1}
}
#goodclassifications/sum(goodclassifications, misclassifications)
```

```{r, echo = FALSE}
goodclassifications/sum(goodclassifications, misclassifications)
```


```{r}
goodclassifications = 0
misclassifications = 0
misclass_per_author = NULL
for (j in 1:2500) {
  Bayes = NULL
  for (i in 1:50) {
    Bayes = append(Bayes, sum(X_test[j,]*w_authors_train[i,]))
  }
  attributed.author = which(Bayes == max(Bayes))
  attributed.author = authors_unique[attributed.author]
  #print(attributed.author)
  if (attributed.author == test.authors[j]) {
    goodclassifications = goodclassifications + 1
  }
  else{misclassifications = misclassifications + 1}
  if (j%%50 == 0) {
    misclass_per_author = append(misclass_per_author, goodclassifications/sum(goodclassifications, misclassifications))
    misclassifications = 0
    goodclassifications = 0
  }
}
mpa.df = data.frame(x = authors_unique, y = misclass_per_author)
mpa.df
```


**Random Forest Model**

Naive Bayes did well predicting authors. Random Forests can also be applied to the same bag of words analysis. In order to do so, the bag of words model needs less variables. We can choose a subset of variables using principal component analysis (PCA). PCA generates 1422 principal components. However, 1422 variables for 2500 observations would produce too much variability error. Thus, we select the top principal components. The number of principal components should be selected using cross validation, for increased accuracy. 
Using 50 principal components, the random forest model is fit to the training data and used to predict on the test data. The percent of correct author attributions obtained is **66%.** This is higher than the Naive Bayes because Random Forest is using more important components dictated by PCA. As a result, the five authors that were difficult to predict are less difficult to predict. shown below is a table of prediction accuracy for each author by Random Forests.


```{r, echo = FALSE}
library(randomForest)
PC_X_train = prcomp(X_train, center = TRUE, scale = TRUE)
pr.var = PC_X_train$sdev^2
pr.ve = pr.var/sum(pr.var)

PC_X_test = prcomp(X_test, center = TRUE, scale = TRUE)


rf.tree = randomForest(x = PC_X_train$x[,1:50], y = factor(authors_train), ntree = 50, mtry = 4)
yhat = predict(rf.tree, newx = PC_X_test$x[,1:50])
sum(yhat == test.authors) / (sum(yhat!= test.authors) + sum(yhat == test.authors))

goodclassifications = 0
misclassifications = 0
misclass_per_author = NULL
for (i in 1:length(yhat)) {
  if (yhat[i] == test.authors[i]) {
    goodclassifications = goodclassifications + 1
  }
  else{misclassifications = misclassifications + 1}
  if (i%%50 == 0) {
    misclass_per_author = append(misclass_per_author, goodclassifications/sum(goodclassifications, misclassifications))
    misclassifications = 0
    goodclassifications = 0
  }
}
mpa.df = data.frame(x = authors_unique, y = misclass_per_author)
mpa.df
```







##Problem 3: Association Rule Mining

Association between grocery items is important for grocery stores because it can help them understand what items are being bought together, which can in tern help them manage supply and appropriately place inventory and promotions. Associations for groceries from groceries.txt were gathered using a minimum support of .01 and confidence of .5. The  associations found are displayed below. This analysis points to two common items, milk and vegetables associated with particular types of products. purchase of milk is associated with eggs and various dairy products, and purchase of "other vegetables"" is associated with purchase of various produce. These all have a support over .01, which means that 10% of all grocery carts contained these associations. They also all have a confidence greater than .5, which means that of grocery carts containing itemset A, over 50% contained the righthand side itemset. 




```{r, echo = FALSE}
library(arules)
groceries = readLines('Data/groceries.txt')
groceries = lapply(groceries, strsplit, split = ',', fixed = TRUE)
groceries = unlist(groceries, recursive = FALSE)
grocerytrans = as(groceries, 'transactions')
basketrules <- apriori(grocerytrans, 
	parameter=list(support=.01, confidence=.5, maxlen=4))
inspect(basketrules)
```














